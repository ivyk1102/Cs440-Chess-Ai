package hw3;

import java.io.InputStream;
import java.io.OutputStream;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Iterator;
import java.util.LinkedList;
import java.util.List;
import java.util.ListIterator;
import java.util.Map;
import java.util.Random;
import java.util.Set;

import edu.bu.hw3.linalg.Matrix;
import edu.bu.hw3.nn.LossFunction;
import edu.bu.hw3.nn.Model;
import edu.bu.hw3.nn.Optimizer;
import edu.bu.hw3.nn.layers.Dense;
import edu.bu.hw3.nn.layers.ReLU;
import edu.bu.hw3.nn.layers.Sigmoid;
import edu.bu.hw3.nn.layers.Tanh;
import edu.bu.hw3.nn.losses.MeanSquaredError;
import edu.bu.hw3.nn.models.Sequential;
import edu.bu.hw3.nn.optimizers.SGDOptimizer;
import edu.bu.hw3.streaming.Streamer;
import edu.bu.hw3.utils.Pair;
import edu.bu.hw3.utils.Triple;
import edu.cwru.sepia.action.Action;
import edu.cwru.sepia.action.ActionFeedback;
import edu.cwru.sepia.action.ActionResult;
import edu.cwru.sepia.agent.Agent;
import edu.cwru.sepia.environment.model.history.DamageLog;
import edu.cwru.sepia.environment.model.history.DeathLog;
import edu.cwru.sepia.environment.model.history.History.HistoryView;
import edu.cwru.sepia.environment.model.state.Unit;
import edu.cwru.sepia.environment.model.state.Unit.UnitView;
import edu.cwru.sepia.environment.model.state.State.StateView;

public class QAgent extends Agent
{

	public static final long serialVersionUID = -5077535504876086643L;
	public static final int RANDOM_SEED = 12345;
	public static final double GAMMA = 0.9;
	public static final double LEARNING_RATE = 0.0001;
	public static final double EPSILON = 0.02; // prob of ignoring the policy and choosing a random action

	// our agent will play this many training episodes in a row before testing
	public static final int NUM_TRAINING_EPISODES_IN_BATCH = 10;

	// our agent will play this many testing episodes in a row before training again
	public static final int NUM_TESTING_EPISODES_IN_BATCH = 5;

	private final String paramFilePath;

	private Streamer streamer;

	private final int NUM_EPISODES_TO_PLAY;

	private int numTestEpisodesPlayedInBatch = -1;
	private int numTrainingEpisodesPlayed = 0;

	// rng to keep things repeatable...will combine with the RANDOM_SEED
	public final Random random;

	private Integer ENEMY_PLAYER_ID; // initially null until initialStep() is called

	private Set<Integer> myUnits;
	private Set<Integer> enemyUnits;
	private List<Double> totalRewards;

	/** NN specific things **/
	private Model qFunctionNN;
	private LossFunction lossFunction;
	private Optimizer optimizer;

	// how we remember what was the state, Q-value, and reward from the past
	private Map<Integer, Triple<Matrix, Matrix, Double> > oldInfoPerUnit;

	private int numDudesOnMyTeam;
	private int numDudesOnEnemyTeam;

	public QAgent(int playerId, String[] args)
	{
		super(playerId);
		String streamerArgString = null;
		String paramFilePath = null;

		if(args.length < 3)
		{
			System.err.println("QAgent.QAgent [ERROR]: need to specify playerId, streamerArgString, paramFilePath");
			System.exit(-1);
		}

		streamerArgString = args[1];
		paramFilePath = args[2];

		int numEpisodesToPlay = QAgent.NUM_TRAINING_EPISODES_IN_BATCH;
		boolean loadParams = false;
		if(args.length >= 4)
		{
			numEpisodesToPlay = Integer.parseInt(args[3]);
			if(args.length >= 5)
			{
				loadParams = Boolean.parseBoolean(args[4]);
			}
		}

		this.NUM_EPISODES_TO_PLAY = numEpisodesToPlay;
		this.ENEMY_PLAYER_ID = null; // initially

		this.paramFilePath = paramFilePath;

		this.myUnits = null;
		this.enemyUnits = null;
		this.totalRewards = new ArrayList<Double>((int)this.NUM_EPISODES_TO_PLAY / QAgent.NUM_TRAINING_EPISODES_IN_BATCH);
		this.totalRewards.add(0.0);

		this.streamer = Streamer.makeDefaultStreamer(streamerArgString, this.getPlayerNumber());
		this.random = new Random(QAgent.RANDOM_SEED);

		this.qFunctionNN = this.initializeQFunction(loadParams);
		this.lossFunction = new MeanSquaredError();
		this.optimizer = new SGDOptimizer(this.getQFunction().getParameters(),
				QAgent.LEARNING_RATE, 10000);
		this.oldInfoPerUnit = new HashMap<Integer, Triple<Matrix, Matrix, Double> >();

		this.numDudesOnMyTeam = 0;
		this.numDudesOnEnemyTeam = 0;
	}

	private final String getParamFilePath() { return this.paramFilePath; }
	private Integer getEnemyPlayerId() { return this.ENEMY_PLAYER_ID; }
	private Set<Integer> getMyUnitIds() { return this.myUnits; }
	private Set<Integer> getEnemyUnitIds() { return this.enemyUnits; }
	private List<Double> getTotalRewards() { return this.totalRewards; }
	private final Streamer getStreamer() { return this.streamer; }
	private final Random getRandom() { return this.random; }
	private int getNumDudesOnMyTeam() { return this.numDudesOnMyTeam; }
	private int getNumDudesOnEnemyTeam() { return this.numDudesOnEnemyTeam; }

	/** NN specific stuff **/
	private Model getQFunction() { return this.qFunctionNN; }
	private LossFunction getLossFunction() { return this.lossFunction; }
	private Optimizer getOptimizer() { return this.optimizer; }
	private Map<Integer, Triple<Matrix, Matrix, Double> > getOldInfoPerUnit() { return this.oldInfoPerUnit; }

	private boolean isTrainingEpisode() { return this.numTestEpisodesPlayedInBatch == -1; }
	
	private void setNumDudesOnMyTeam(int val) { this.numDudesOnMyTeam = val; }
	private void setNumDudesOnEnemyTeam(int val) { this.numDudesOnEnemyTeam = val; }

	/**
	 * A method to create the neural network used for the Q function.
	 * You can make it as deep as you want to (although it will take more time to compute)
	 * 
	 * The API for creating a neural network is as follows:
	 *     Sequential m = new Sequential();
	 *     // layer 1
	 *     m.add(new Dense(feature_dim, hidden_dim1, this.getRandom()));
	 *     m.add(Sigmoid()); //activation function
	 *     
	 *     // layer 2
	 *     m.add(new Dense(hidden_dim1, hidden_dim2, this.getRandom()));
	 *     m.add(Tanh()); 
	 *     
	 *     // add as many layers as you want
	 *     
	 *     // the last layer MUST be a scalar though
	 *     m.add(new Dense(hidden_dimN, 1));
	 *     m.add(ReLU()); // decide if you want to add an activation
	 * 
	 * @param loadParams
	 * @return
	 */
	private Model initializeQFunction(boolean loadParams)
	{
		Sequential m = new Sequential(); 
		//inner layer
		m.add(new Dense(5, 10, this.getRandom()));
        m.add(new Sigmoid());

        // layer 2
        m.add(new Dense(10, 20, this.getRandom()));
        m.add(new Tanh());

        // layer 3
        m.add(new Dense(20, 1, this.getRandom()));
        m.add(new Tanh()); //will now take negative values as well. bounded

		/**
		 * TODO: create your model!
		 * 
		 * 10 features = = 10 units as input
		 * layers, activation, units .
		 * input layer = 
		 * last layer = output layer outputs scalar val = 1
		 * claucalte the features == rho 1 by 10. (calcuatefeature vector function)
		 * no layers, returning a null rn 
		 * figure out teh reward/punish&reward controls what runs 
		 * kill enemy = reward
		 * die unit == punishment
		 * 
		 */ 

		if(loadParams)
		{
			try
			{
				m.load(this.getParamFilePath());
			} catch (Exception e)
			{
				// TODO Auto-generated catch block
				e.printStackTrace();
				System.exit(-1);
			}
		}
		return m;
	}

	/**
     * Given the current state and the footman in question calculate the reward received on the last turn.
     * This is where you will check for things like Did this footman take or give damage? Did this footman die
     * or kill its enemy. Did this footman start an action on the last turn? 
     *
     * Remember that you will need to discount this reward based on the timestep it is received on.
     *
     * As part of the reward you will need to calculate if any of the units have taken damage. You can use
     * the history view to get a list of damages dealt in the previous turn. Use something like the following.
     *
     * for(DamageLog damageLogs : historyView.getDamageLogs(lastTurnNumber)) {
     *     System.out.println("Defending player: " + damageLog.getDefenderController() + " defending unit: " + \
     *         damageLog.getDefenderID() + " attacking player: " + damageLog.getAttackerController() + \
     *         "attacking unit: " + damageLog.getAttackerID());
     * }
     *
     * You will do something similar for the deaths. See the middle step documentation for a snippet
     * showing how to use the deathLogs.
     *
     * To see if a command was issued you can check the commands issued log.
     *
     * Map<Integer, Action> commandsIssued = historyView.getCommandsIssued(playernum, lastTurnNumber);
     * for (Map.Entry<Integer, Action> commandEntry : commandsIssued.entrySet()) {
     *     System.out.println("Unit " + commandEntry.getKey() + " was command to " + commandEntry.getValue().toString);
     * }
     *
     * @param state The current state of the game.
     * @param history History of the episode up until this turn.
     * @param unitId The id of the unit you are looking to calculate the reward for.
     * @return The current rew ard for that unit
     */
    private double getRewardForUnit(StateView state, HistoryView history, int unitId)
    {
    	//TODO: complete me!
    	double reward = 0;
        int playerId = this.getPlayerNumber(); //
        int lastTurnNumber = state.getTurnNumber() - 1;

        // Kill good
        // Killed bad
        // Attack good
        // Attacked bad

        for(DamageLog damageLogs : history.getDamageLogs(lastTurnNumber)) {
            int AttackingUnit = damageLogs.getAttackerController();
            if (AttackingUnit == playerId) { // We are the attacking unit
                reward += 1000;
            } else {
                reward -= 1000; // We got attacked
            }
//            System.out.println("Defending player: " + damageLogs.getDefenderController() + " defending unit: " +
//                         damageLogs.getDefenderID() + " attacking player: " + damageLogs.getAttackerController() +
//              "attacking unit: " + damageLogs.getAttackerID());
        }

        for(DeathLog deathLog : history.getDeathLogs(lastTurnNumber)) {
            if(deathLog.getController() == playerId)
            {
                reward -= 3000; // we were killed 😦
            }
            else if(deathLog.getController() == this.getEnemyPlayerId())
            {
                reward += 3000; // we slaying 🙂
            }
        }
        return reward;
    }

    
   
    /**
    * Given a state and action calculate your features here. Please include a comment explaining what features
    * you chose and why you chose them.
    *
    *Q value = an estimation of how good an action is (scalar value) PREDICTS. HOW GOOD IS THIS ACTION IN THIS STATE
    *then it runs 
    *
    *AFTER ACTION COMPLETES : ESTIMATION VALUE  AND ACTION VALUE 
    *
    *q FUNCTION == we implement . 
    *we have description of each elemetn of a grander scheme
    *
    *
    *processing state and action together 
    *
    *neural network only takes in a vector (bunch of numbers)
    *we must convert the state into numebrs 
    *we must convert action into numbers
    *
    *WE MUST DECIDE WHAT EACH NUMBER WILL REPRESENT 
    *what ways will we measure each state 
    *
    *
    *Q function == estimates rewards (sorta)
    *will choose the action+state with the best value (i believe his code does this?) !!!
    *everything is arbitrary. damage, the number of agents you are vsing etc etc 
    *base health base damage etc etc. 
    *EVERY FEATURE MUST BE INVARIANT. NORMALIZE THEM . ******MAKE PERCENTAGE 
    *field == base health == field for current health 
    *unitView = current health. currentView = base view. Sepia has documentation on it
    *
    *
    *
    *NEURAL NETWORK  == 
    *gets fed into input layer 
    *values flow into the edges (preactivation?) 
    *then goes into activation 
    *
    *
    *give neural network == the feature vector!!!!!!!!!!!!!
    *create the layers, he made the code to make it play it out
    *ASSEMBLE THE LEGOS, HIS CODE BUILDS IT
    *
    *
    *
    *GOAL: which thingymabobby is going to attack which thingymabobby? 
    *
    * closer to the present? we should priotize more vs the favorable things in the future. 
    * we must deflate their utility. we dont have to change it. ONLY CHANGE his code if necessary
    * 
    * reward == andrew's code calculates gradience 
    *
    *
    *
    * All of your feature functions should evaluate to a double. Collect all of these into a row vector
    * (a Matrix with 1 row and n columns). This will be the input to your neural network
    *
    * It is a good idea to make the first value in your array a constant. This just helps remove any offset
    * from 0 in the Q-function. The other features are up to you.
    * 
    * It might be a good idea to save whatever feature vector you calculate in the oldFeatureVectors field
    * so that when that action ends (and we observe a transition to a new state), we can update the Q value Q(s,a)
    *
    * @param state Current state of the SEPIA game
    * @param history History of the game up until this turn
    * @param atkUnitId Your unit. The one doing the attacking.
    * @param tgtUnitId An enemy unit. The one your unit is considering attacking.
    * @return The Matrix of feature function outputs.
    */
   private Matrix calculateFeatureVector(StateView state, HistoryView history,
                                        int atkUnitId, int tgtUnitId)
   		//damage dealt and damage taken 
   //kill == very good death == very bad 
   //dont 
   //percentage of people on your team
   // dont separate from the pack
   //normalize & make ti resilient to the changes in the world
   //ratio of our units base health against how much damage do they do? 
   //MAX HP our unit can have against how much damage does it do ? (
   //glass cannon? pool noodle (low defense, high attack) (high defense, low attack)
   //team configurations can change but will be constant throughout . just depends
   //learning on a fair game. 
   //NEEDS TO BE INVARIANT
   //ask to rank a bunch of actions
   //neural network produces ranking 
   //after choose action, action will resolve. complete succcess or fail. unit 2 vs 5, might not be possible cuz 5 might die before
   //once action resolves, we can measure how good that action was (reward) 
   //describing the footman to the neural network
   //describing the enemy footman to the neural network and its actions
   //select action chooses this action andrews code already provides it 
   //translating the sepia state and action into how the neural network perceives it and his code translates it and will
   //give us the best action the network should chosoe 
   
   
   ///create each method for each feature
   
   //WHAT CAN IT SEE
   
   
   
   //NEED TO CREATE THE ROW VECTOR T_T can figure out tmw
   			
   {
  	 	Matrix curMat = Matrix.zeros(1, 5);
  		curMat.set(0,0,1); //where 1 is our constant in the first position. 
  		curMat.set(0,1, calculateRatioOfOurTeam());
  		curMat.set(0,2, calculateRatioOfEnemyTeam());
  		curMat.set(0,3, euclidDist(state, atkUnitId, tgtUnitId));
  		curMat.set(0, 4, attackTheWeak(state, atkUnitId, tgtUnitId));
  		
  			//layer number, //number of units
  		//what do we do with this ratio value?
	   
	   /** TODO: complete me! **/
	   return curMat;
   }
   
   
   private double attackTheWeak(StateView state, int atkUnitId, int tgtUnitId) {
	   UnitView attackingUnit = state.getUnit(atkUnitId);
	   UnitView targetUnit = state.getUnit(tgtUnitId);
	   double attackingHP = attackingUnit.getHP();
       double targetHP = targetUnit.getHP();
	   
       return attackingHP-targetHP;
       
   }
   
   
   // How far units are from each other
   private double euclidDist(StateView state, int atkUnitId, int tgtUnitId) {
	   UnitView attackingUnit = state.getUnit(atkUnitId);
	   UnitView targetUnit = state.getUnit(tgtUnitId);
	   double xDiff = attackingUnit.getXPosition() - targetUnit.getXPosition();
	   double yDiff = attackingUnit.getYPosition() - targetUnit.getYPosition();
   // Euclidean Distance
	   double dist = Math.sqrt(xDiff * xDiff + yDiff * yDiff);
	   return dist;
   }
   
   
   private double calculateRatioOfOurTeam() {
	   double ratio_of_our_alive = this.getMyUnitIds().size() / this.getNumDudesOnMyTeam(); 
	   return ratio_of_our_alive;
   }
   
   private double calculateRatioOfEnemyTeam() {
	   double ratio_of_our_alive = this.getEnemyUnitIds().size() / getNumDudesOnEnemyTeam(); 
	   return ratio_of_our_alive;
   }
   
    /**
     * Calculate the Q-Value for a given state action pair. The state in this scenario is the current
     * state view and the history of this episode. The action is the attacker and the enemy pair for the
     * SEPIA attack action.
     *
     * This returns the Q-value according to your feature approximation. This is where you will pass
     * your features through your network (and extract the predicted q-value using the .item() method)
     * @param featureVec The feature vector
     * @return The approximate Q-value
     */
    private double calculateQValue(Matrix featureVec)
    {
    	double qValue = 0.0;
        try
        {
			qValue = this.getQFunction().forward(featureVec).item();
		} catch (Exception e)
        {
			System.err.println("QAgent.caculateQValue [ERROR]: error in either forward() or item()");
			e.printStackTrace();
			System.exit(-1);
		}
        return qValue;
    }

    /**
     * Given a unit and the current state and history of the game select the enemy that this unit should
     * attack. This is where you would do the epsilon-greedy action selection.
     * 
     * You will need to consider who to attack. A unit should always be attacking
     * (if it is not currently attacking something), so what makes actions "different"
     * is who the unit is attacking
     *
     * @param state Current state of the game
     * @param history The entire history of this episode
     * @param atkUnitId The unit (your unit) that will be attacking
     * @return The enemy footman ID this unit should attack
     */
    private int selectAction(StateView state, HistoryView history, int atkUnitId)
    {
    	Integer tgtUnitId = null;
    	Matrix featureVec = null;
    	double maxQ = Double.NEGATIVE_INFINITY;
    	double r = this.getRewardForUnit(state, history, atkUnitId);

    	// epsilon-greedy (i.e. random exploration function)
    	if(this.getRandom().nextDouble() < QAgent.EPSILON && this.isTrainingEpisode())
    	{
    		// ignore policy and choose a random action (i.e. attacking which enemy)
    		int randomEnemyIdx = this.getRandom().nextInt(this.getEnemyUnitIds().size());

    		// get the unitId at that position
    		tgtUnitId = this.getEnemyUnitIds().toArray(new Integer[this.getEnemyUnitIds().size()])[randomEnemyIdx];
    		featureVec = this.calculateFeatureVector(state, history, atkUnitId, tgtUnitId);
    		maxQ = this.calculateQValue(featureVec);
    	} else
    	{
	    	// find the action (i.e. attacking which enemy) that maximizes the Q-value
	    	for(Integer enemyUnitId : this.getEnemyUnitIds())
	    	{
	    		Matrix features = this.calculateFeatureVector(state, history, atkUnitId, enemyUnitId);
	    		double qValue = this.calculateQValue(features);
	
	    		if(qValue > maxQ)
	    		{
	    			maxQ = qValue;
	    			featureVec = features;
	    			tgtUnitId = enemyUnitId;
	    		}
	    	}
    	}

    	// remember the info for this unit
    	this.getOldInfoPerUnit().put(atkUnitId, new Triple<Matrix, Matrix, Double>(featureVec, Matrix.full(1, 1, maxQ), r));

    	return tgtUnitId;
    }

    /**
     * This method calculates what the "true" Q(s,a) value should have been based on the Bellman equation for Q-values
     *
     * @param state The current state of the game
     * @param history The current history of the game
     * @param unitId The friendly unitId under consideration
     * @return
     */
    private Matrix getTDGroundTruth(StateView state, HistoryView history, int unitId) throws Exception
    {
    	if(!this.getOldInfoPerUnit().containsKey(unitId))
    	{
    		throw new Exception("unitId=" + unitId + " does not have an old feature vector...cannot calculate TD ground truth for it");
    	}
    	Triple<Matrix, Matrix, Double> oldInfo = this.getOldInfoPerUnit().get(unitId);
    	Double Rs = oldInfo.getThird();

    	double maxQ = Double.NEGATIVE_INFINITY;

    	// try all the actions (i.e. who to attack) in the current state
    	for(Integer tgtUnitId: this.getEnemyUnitIds())
    	{
    		maxQ = Math.max(maxQ, this.calculateQValue(this.calculateFeatureVector(state, history, unitId, tgtUnitId)));
    	}

    	return Matrix.full(1, 1, Rs + QAgent.GAMMA*maxQ); // output is always a scalar in active learning
    }

    /**
     * Calculate the updated weights for this agent. You should construct a matrix
     * @param r The reward R(s) for the prior state
     * @param state Current state of the game.
     * @param history History of the game up until this point
     * @param unitId The unit under consideration
     */
    private void updateParams(StateView state, HistoryView history, int unitId) throws Exception
    {
    	if(!this.getOldInfoPerUnit().containsKey(unitId))
    	{
    		throw new Exception("unitId=" + unitId + " does not have an old feature vector...cannot update params for it");
    	}
    	Triple<Matrix, Matrix, Double> oldInfo = this.getOldInfoPerUnit().get(unitId);
    	Matrix oldFeatureVector = oldInfo.getFirst();
    	Matrix Qsa = oldInfo.getSecond();

    	// reset the optimizer (i.e. reset gradients)
    	this.getOptimizer().reset();

    	// populate gradients
    	this.getQFunction().backwards(oldFeatureVector, this.getLossFunction().backwards(Qsa, this.getTDGroundTruth(state, history, unitId)));

    	// take a step in the correct direction
    	this.getOptimizer().step();
    }


	@Override
	public Map<Integer, Action> initialStep(StateView state, HistoryView history)
	{
		// find who our unitIDs are
		this.myUnits = new HashSet<Integer>();
		for(Integer unitId: state.getUnitIds(this.getPlayerNumber()))
		{
			UnitView unitView = state.getUnit(unitId);
			// System.out.println("Found new unit for player=" + this.getPlayerNumber() + " of type=" + unitView.getTemplateView().getName().toLowerCase() + " (id=" + unitId + ")");

			this.myUnits.add(unitId);
		}

		// find the enemy player
		Set<Integer> playerIds = new HashSet<Integer>();
		for(Integer playerId: state.getPlayerNumbers())
		{
			playerIds.add(playerId);
		}
		if(playerIds.size() != 2)
		{
			System.err.println("QAgent.initialStep [ERROR]: expected two players");
			System.exit(-1);
		}
		playerIds.remove(this.getPlayerNumber());
		this.ENEMY_PLAYER_ID = playerIds.iterator().next(); // get first element

		this.enemyUnits = new HashSet<Integer>();
		for(Integer unitId: state.getUnitIds(this.getEnemyPlayerId()))
		{
			UnitView unitView = state.getUnit(unitId);
			// System.out.println("Found new unit for player=" + this.getEnemyPlayerId() + " of type=" + unitView.getTemplateView().getName().toLowerCase() + " (id=" + unitId + ")");

			this.enemyUnits.add(unitId);
		}

		Integer enemyID = this.getEnemyPlayerId();
		int enemyNum = enemyID.intValue();
		List<Integer> enemyUnits = state.getUnitIds(enemyNum);
		
		this.setNumDudesOnMyTeam(state.getUnitIds(this.getPlayerNumber()).size());
		this.setNumDudesOnEnemyTeam(enemyUnits.size());
		return this.middleStep(state, history);
	}

	/**
     * You will need to calculate the reward at each step and update your totals. You will also need to
     * check if an event has occurred. If it has then you will need to update your weights and select a new action.
     *
     * If you are using the footmen vectors you will also need to remove killed units. To do so use the historyView
     * to get a DeathLog. Each DeathLog tells you which player's unit died and the unit ID of the dead unit. To get
     * the deaths from the last turn do something similar to the following snippet. Please be aware that on the first
     * turn you should not call this as you will get nothing back.
     *
     * for(DeathLog deathLog : historyView.getDeathLogs(stateView.getTurnNumber() -1)) {
     *     System.out.println("Player: " + deathLog.getController() + " unit: " + deathLog.getDeadUnitID());
     * }
     *
     * You should also check for completed actions using the history view. Obviously you never want a footman just
     * sitting around doing nothing (the enemy certainly isn't going to stop attacking). So at the minimum you will
     * have an event whenever one your footmen's targets is killed or an action fails. Actions may fail if the target
     * is surrounded or the unit cannot find a path to the unit. To get the action results from the previous turn
     * you can do something similar to the following. Please be aware that on the first turn you should not call this
     *
     * Map<Integer, ActionResult> actionResults = historyView.getCommandFeedback(playernum, stateView.getTurnNumber() - 1);
     * for(ActionResult result : actionResults.values()) {
     *     System.out.println(result.toString());
     * }
     *
     * @return New actions to execute or nothing if an event has not occurred.
     */
	@Override
	public Map<Integer, Action> middleStep(StateView state, HistoryView history)
	{
		Map<Integer, Action> actions = new HashMap<Integer, Action>(this.getMyUnitIds().size());

    	// if this isn't the first turn in the game
    	if(state.getTurnNumber() > 0)
    	{

    		// check death logs and remove dead units
    		//removes all dead units from the set of unitIds
    		for(DeathLog deathLog : history.getDeathLogs(state.getTurnNumber() - 1))
    		{
    			if(deathLog.getController() == this.getPlayerNumber())
    			{
    				this.getMyUnitIds().remove(deathLog.getDeadUnitID());
    			}
    			else if(deathLog.getController() == this.getEnemyPlayerId())
    			{
    				this.getEnemyUnitIds().remove(deathLog.getDeadUnitID());
    			}
    		}
    	}

    	// get the previous action history in the previous step
		Map<Integer, ActionResult> prevUnitActions = history.getCommandFeedback(this.playernum, state.getTurnNumber() - 1);

    	for(Integer unitId : this.getMyUnitIds())
    	{
    		// decide what each unit should do (i.e. attack)

    		// calculate the reward for this unit
    		double reward = this.getRewardForUnit(state, history, unitId);

    		// if we are playing a test episode then add these rewards to the total reward for the test games
    		if(this.numTestEpisodesPlayedInBatch != -1)
    		{
    			this.totalRewards.set(this.totalRewards.size() - 1, 
    				this.totalRewards.get(this.totalRewards.size() - 1) + Math.pow(this.GAMMA, state.getTurnNumber() - 1) * reward);
    		}
    		
    		//if this unit does not have an action or the action was completed or failed...give a unit an action
    		if(state.getTurnNumber() == 0 || !prevUnitActions.containsKey(unitId) || 
    				prevUnitActions.get(unitId).getFeedback().equals(ActionFeedback.COMPLETED) ||
    				prevUnitActions.get(unitId).getFeedback().equals(ActionFeedback.FAILED))
    		{
    			if(state.getTurnNumber() > 0)
    			{
    				// we have arrived at a new state for that unit, so time to update some gradients
    				try
    				{
						this.updateParams(state, history, unitId);
					} catch (Exception e)
    				{
						System.err.println("QAgent.middleStep [ERROR]: problem updating gradients for transition on unitId=" + unitId);
						e.printStackTrace();
						System.exit(-1);
					}
    			}
    			int tgtUnitId = this.selectAction(state, history, unitId);
    			actions.put(unitId, Action.createCompoundAttack(unitId, tgtUnitId));
    		}
    	}

    	if(actions.size() > 0)
    	{
    		this.getStreamer().streamMove(actions);
    	}
        return actions;
	}

	@Override
	public void terminalStep(StateView state, HistoryView history)
	{
		if(this.isTrainingEpisode())
		{
			// save the model
			this.getQFunction().save(this.getParamFilePath());

			this.numTrainingEpisodesPlayed += 1;
			if((this.numTrainingEpisodesPlayed % QAgent.NUM_TRAINING_EPISODES_IN_BATCH) == 0)
			{
				this.numTestEpisodesPlayedInBatch = 0;
			}
		} else
		{
			this.numTestEpisodesPlayedInBatch += 1;
			if((this.numTestEpisodesPlayedInBatch % QAgent.NUM_TESTING_EPISODES_IN_BATCH) == 0)
			{
				this.numTestEpisodesPlayedInBatch = -1;
				// calculate the average
				this.getTotalRewards().set(this.getTotalRewards().size()-1,
						this.getTotalRewards().get(this.getTotalRewards().size()-1) / QAgent.NUM_TRAINING_EPISODES_IN_BATCH);
	
				// print the average test rewards
				this.printTestData(this.getTotalRewards());
	
				if(this.numTrainingEpisodesPlayed == this.NUM_EPISODES_TO_PLAY)
				{
					System.out.println("played all " + this.NUM_EPISODES_TO_PLAY + " games!");
					System.exit(0);
				} else
				{
					this.getTotalRewards().add(0.0);
				}
			}
		}
	}

	/**
     * DO NOT CHANGE THIS!
     *
     * Prints the learning curve data described in the assignment. Do not modify this method.
     *
     * @param averageRewards List of cumulative average rewards from test episodes.
     */
    private void printTestData (List<Double> averageRewards)
    {
        System.out.println("");
        System.out.println("Games Played      Average Cumulative Reward");
        System.out.println("-------------     -------------------------");
        for (int i = 0; i < averageRewards.size(); i++)
        {
            String gamesPlayed = Integer.toString(QAgent.NUM_TRAINING_EPISODES_IN_BATCH*(i+1));
            String averageReward = String.format("%.2f", averageRewards.get(i));

            int numSpaces = "-------------     ".length() - gamesPlayed.length();
            StringBuffer spaceBuffer = new StringBuffer(numSpaces);
            for (int j = 0; j < numSpaces; j++)
            {
                spaceBuffer.append(" ");
            }
            System.out.println(gamesPlayed + spaceBuffer.toString() + averageReward);
        }
        System.out.println("");
    }

	@Override
	public void loadPlayerData(InputStream inStream) {}

	@Override
	public void savePlayerData(OutputStream outStream) {}

}
